{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 11:25:40.064047: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-07 11:25:40.090230: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-07 11:25:40.466833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-07-07 11:25:42.771619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 11:25:42.786710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 11:25:42.786898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 11:25:42.787656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 11:25:42.787807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 11:25:42.787917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 11:25:43.136599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 11:25:43.136783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 11:25:43.136894: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 11:25:43.136990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 151 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"MRC_LSTM.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1D0cveLabP1nYyB1wGipzScvtj7ybbd0S\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"## Imports and multi-worker config\"\"\"\n",
    "\n",
    "\n",
    "from preprocessing import process\n",
    "from model import create_model\n",
    "from contextlib import redirect_stdout\n",
    "from plotting import save_loss_accuracy_plots\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import tensorflow as tf\n",
    "import io\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "\n",
    "\"\"\"## Constants\"\"\"\n",
    "\n",
    "model_params_file = \"./parameters/params.json\"\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Load parameters from the JSON file\n",
    "with open(model_params_file, 'r') as file:\n",
    "    params = json.load(file)\n",
    "\n",
    "# Access the parameters\n",
    "filename = params['filename']\n",
    "window_size = params['window_size']\n",
    "num_features = params['num_features']\n",
    "batch_size = params['batch_size']\n",
    "epochs = params['epochs']\n",
    "initial_learning_rate = params['initial_learning_rate']\n",
    "filter_size = params['filter_size']\n",
    "external_filter_size = params['external_filter_size']\n",
    "l2_reg = params['l2_reg']\n",
    "bidirectional = params['bidirectional']\n",
    "interval = params['interval']\n",
    "train_size = params['train_size']\n",
    "validation_size = params['validation_size']\n",
    "test_size = params['test_size']\n",
    "nr_of_labels = params['nr_of_labels']\n",
    "\n",
    "assert (train_size + test_size + validation_size) == 1\n",
    "\n",
    "base_results_path = './results'\n",
    "\n",
    "\"\"\"## Loading data from CSV\"\"\"\n",
    "\n",
    "data = pd.read_csv(filename)\n",
    "\n",
    "\"\"\"## Preprocessing\"\"\"\n",
    "\n",
    "data = process(data, interval, False)\n",
    "\n",
    "\"\"\"## Extracting features and labels\"\"\"\n",
    "\n",
    "data = data[['Coppock', 'RSI', 'StochRSI', 'ROC', 'MACD_Line', 'Signal_Line', 'MACD', 'null', '-null', 'long', 'short']]\n",
    "\n",
    "\"\"\"## Reshaping to fit convolutional layers\"\"\"\n",
    "\n",
    "num_samples = len(data) - window_size + 1\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "# window[from 1 to windowsize -1, all columns except for the last two] window[only the last one, last two columns]\n",
    "dataset = dataset.map(lambda window: (window[:-1, :-nr_of_labels], tf.squeeze(window[-1:, -nr_of_labels:])))\n",
    "\n",
    "\"\"\"## Split the data\"\"\"\n",
    "\n",
    "# One time data shuffle\n",
    "shuffle_buffer_size = num_samples\n",
    "# dataset = dataset.shuffle(shuffle_buffer_size, reshuffle_each_iteration=False) # hope fixed\n",
    "\n",
    "# Splitting into training and validation sets\n",
    "train_size = int(train_size * num_samples)\n",
    "validation_size = int(validation_size * num_samples)\n",
    "test_size = int(test_size * num_samples)\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size).take(validation_size)\n",
    "test_dataset = dataset.skip(train_size).skip(validation_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in dataset:\n",
    " print(\"Feature:\", X.numpy(), \"Target:\", y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Data shuffle on each iteration\n",
    "shuffle_buffer_size = int(num_samples * train_size)\n",
    "# train_dataset = train_dataset.shuffle(shuffle_buffer_size, reshuffle_each_iteration=False) # hope fixed\n",
    "\n",
    "# Grouping windows into batches\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "\"\"\"## Model object and strategy\"\"\"\n",
    "\n",
    "model = create_model(window_size, num_features, bidirectional, filter_size, l2_reg, external_filter_size, nr_of_labels)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=initial_learning_rate\n",
    ")\n",
    "\n",
    "if nr_of_labels == 2:\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "else:\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'], run_eagerly=True)\n",
    "\n",
    "\"\"\"## Logging, saving and plotting\"\"\"\n",
    "\n",
    "run_path = os.path.join(base_results_path, timestamp)\n",
    "os.makedirs(run_path)\n",
    "\n",
    "# Model summary\n",
    "summary_file = os.path.join(run_path, 'model_summary.txt')\n",
    "\n",
    "buffer = io.StringIO()\n",
    "with redirect_stdout(buffer):\n",
    "    model.summary()\n",
    "summary_string = buffer.getvalue()\n",
    "\n",
    "with open(summary_file, 'w') as file:\n",
    "    model_summary = model.summary()\n",
    "\n",
    "    file.write(summary_string)\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint_path = os.path.join(run_path, 'checkpoints')\n",
    "os.makedirs(checkpoint_path)\n",
    "checkpoint_file_path = os.path.join(checkpoint_path, 'checkpoint.h5')\n",
    "\n",
    "# Logs\n",
    "logs_path = os.path.join(run_path, \"logs\")\n",
    "os.makedirs(logs_path)\n",
    "logs_file_path = os.path.join(logs_path, \"logs.log\")\n",
    "\n",
    "logging.basicConfig(filename=logs_file_path, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.FileHandler(logs_file_path))\n",
    "\n",
    "# Training progress\n",
    "progress_path = os.path.join(run_path, \"progress\")\n",
    "os.makedirs(progress_path)\n",
    "progress_file_path = os.path.join(progress_path, \"progress.log\")\n",
    "\n",
    "# Plotting\n",
    "plot_path = os.path.join(run_path, \"plots\")\n",
    "os.makedirs(plot_path)\n",
    "\n",
    "# Parameters\n",
    "params_path = os.path.join(run_path, \"params\")\n",
    "os.makedirs(params_path)\n",
    "params_path_file = os.path.join(params_path, \"params.json\")\n",
    "command = f'cp {model_params_file} {params_path_file}'\n",
    "os.popen(command)\n",
    "\n",
    "\"\"\"## Callbacks\"\"\"\n",
    "\n",
    "# Define the learning rate schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=[250, 500, 1000, 1500],\n",
    "    values=[initial_learning_rate, initial_learning_rate * 0.3, initial_learning_rate * 0.3 * 0.3,\n",
    "            initial_learning_rate * 0.3 * 0.3 * 0.3, initial_learning_rate * 0.3 * 0.3 * 0.3 * 0.3]\n",
    ")\n",
    "\n",
    "# Set the early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)  # zmienic na 5\n",
    "\n",
    "# Define a custom callback to clear GPU memory\n",
    "\n",
    "\n",
    "class ClearMemoryCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "# Create an instance of the custom callback\n",
    "clear_memory_callback = ClearMemoryCallback()\n",
    "\n",
    "# Set checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_file_path,\n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch',\n",
    "    period=5  # Save every 5 epochs\n",
    ")\n",
    "\n",
    "# Create the CSVLogger callback to save training logs\n",
    "csv_logger_callback = CSVLogger(progress_file_path)\n",
    "\n",
    "\"\"\"## Learning process\"\"\"\n",
    "\n",
    "# Devices\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "\n",
    "# Train the model with learning rate schedule and early stopping\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=epochs, callbacks=[  # tf.keras.callbacks.LearningRateScheduler(lr_schedule),\n",
    "    clear_memory_callback, early_stopping, checkpoint_callback, csv_logger_callback])\n",
    "\n",
    "\"\"\"## Plotting anf saving\"\"\"\n",
    "\n",
    "############################ Test ###############################\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Saving\n",
    "model_path = os.path.join(run_path, 'model')\n",
    "os.makedirs(model_path)\n",
    "model_file_path = os.path.join(model_path, 'model.h5')\n",
    "model.save(model_file_path)\n",
    "\n",
    "# Plotting\n",
    "save_loss_accuracy_plots(history, plot_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
