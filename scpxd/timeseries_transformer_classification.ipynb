{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Timeseries classification with a Transformer model\n",
        "\n",
        "**Author:** [Theodoros Ntakouris](https://github.com/ntakouris)<br>\n",
        "**Date created:** 2021/06/25<br>\n",
        "**Last modified:** 2021/08/05<br>\n",
        "**Description:** This notebook demonstrates how to do timeseries classification using a Transformer model."
      ],
      "metadata": {
        "id": "nqI3nppFUnRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This is the Transformer architecture from\n",
        "[Attention Is All You Need](https://arxiv.org/abs/1706.03762),\n",
        "applied to timeseries instead of natural language.\n",
        "\n",
        "This example requires TensorFlow 2.4 or higher.\n",
        "\n",
        "## Load the dataset\n",
        "\n",
        "We are going to use the same dataset and preprocessing as the\n",
        "[TimeSeries Classification from Scratch](https://keras.io/examples/timeseries/timeseries_classification_from_scratch)\n",
        "example."
      ],
      "metadata": {
        "id": "-ETXrtFkUnRo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def readucr(filename):\n",
        "    data = np.loadtxt(filename, delimiter=\"\\t\")\n",
        "    y = data[:, 0]\n",
        "    x = data[:, 1:]\n",
        "    return x, y.astype(int)\n",
        "\n",
        "\n",
        "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
        "\n",
        "x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
        "x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n",
        "\n",
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "print(x_train)\n",
        "exit()\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "n_classes = len(np.unique(y_train))\n",
        "\n",
        "idx = np.random.permutation(len(x_train))\n",
        "x_train = x_train[idx]\n",
        "y_train = y_train[idx]\n",
        "\n",
        "y_train[y_train == -1] = 0\n",
        "y_test[y_test == -1] = 0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[-0.79717168]\n",
            "  [-0.66439208]\n",
            "  [-0.37301463]\n",
            "  ...\n",
            "  [-0.66439208]\n",
            "  [-1.0737958 ]\n",
            "  [-1.5643427 ]]\n",
            "\n",
            " [[ 0.80485472]\n",
            "  [ 0.63462859]\n",
            "  [ 0.37347448]\n",
            "  ...\n",
            "  [-0.71488505]\n",
            "  [-0.56044294]\n",
            "  [-0.31908642]]\n",
            "\n",
            " [[ 0.7279851 ]\n",
            "  [ 0.11128392]\n",
            "  [-0.49912439]\n",
            "  ...\n",
            "  [ 0.39446303]\n",
            "  [ 0.33940042]\n",
            "  [ 0.25539062]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.57005428]\n",
            "  [-0.33316523]\n",
            "  [-0.29351853]\n",
            "  ...\n",
            "  [-1.3937145 ]\n",
            "  [-0.94273327]\n",
            "  [-0.27072168]]\n",
            "\n",
            " [[ 2.0067321 ]\n",
            "  [ 2.0791499 ]\n",
            "  [ 2.0220362 ]\n",
            "  ...\n",
            "  [-0.43214504]\n",
            "  [-0.44123126]\n",
            "  [-0.28070891]]\n",
            "\n",
            " [[-0.12524091]\n",
            "  [-0.32536268]\n",
            "  [-0.48823697]\n",
            "  ...\n",
            "  [ 0.55576053]\n",
            "  [ 0.57445102]\n",
            "  [ 0.57311598]]]\n"
          ]
        }
      ],
      "metadata": {
        "id": "lIGYp-sKUnRp",
        "outputId": "3482c9e8-7e4d-427c-a911-649491d4d3c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "input_shape"
      ],
      "metadata": {
        "id": "RjRlQ3tZUvQ6",
        "outputId": "54c00517-a2c7-4824-d9a0-34e31a996eb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "Our model processes a tensor of shape `(batch size, sequence length, features)`,\n",
        "where `sequence length` is the number of time steps and `features` is each input\n",
        "timeseries.\n",
        "\n",
        "You can replace your classification RNN layers with this one: the\n",
        "inputs are fully compatible!"
      ],
      "metadata": {
        "id": "AdGR0pgAUnRp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "outputs": [],
      "metadata": {
        "id": "IYzENSKoUnRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We include residual connections, layer normalization, and dropout.\n",
        "The resulting layer can be stacked multiple times.\n",
        "\n",
        "The projection layers are implemented through `keras.layers.Conv1D`."
      ],
      "metadata": {
        "id": "22tmxubcUnRq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "xzhmfv_TUnRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main part of our model is now complete. We can stack multiple of those\n",
        "`transformer_encoder` blocks and we can also proceed to add the final\n",
        "Multi-Layer Perceptron classification head. Apart from a stack of `Dense`\n",
        "layers, we need to reduce the output tensor of the `TransformerEncoder` part of\n",
        "our model down to a vector of features for each data point in the current\n",
        "batch. A common way to achieve this is to use a pooling layer. For\n",
        "this example, a `GlobalAveragePooling1D` layer is sufficient."
      ],
      "metadata": {
        "id": "BkLeYlmkUnRr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
        "    return keras.Model(inputs, outputs)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "cjVJyjgSUnRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and evaluate"
      ],
      "metadata": {
        "id": "thr-jmsUUnRt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
        "\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=200,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "model.evaluate(x_test, y_test, verbose=1)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-51d187a333df>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model = build_model(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhead_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'build_model' is not defined"
          ]
        }
      ],
      "metadata": {
        "id": "G_CWgBv4UnRt",
        "outputId": "2b49e240-80e4-438b-a29b-0c58f9daaeec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "\n",
        "In about 110-120 epochs (25s each on Colab), the model reaches a training\n",
        "accuracy of ~0.95, validation accuracy of ~84 and a testing\n",
        "accuracy of ~85, without hyperparameter tuning. And that is for a model\n",
        "with less than 100k parameters. Of course, parameter count and accuracy could be\n",
        "improved by a hyperparameter search and a more sophisticated learning rate\n",
        "schedule, or a different optimizer.\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/timeseries_transformer_classification) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/timeseries_transformer_classification)."
      ],
      "metadata": {
        "id": "ZnjdBzg6UnRu"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "timeseries_transformer_classification",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}