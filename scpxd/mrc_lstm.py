# -*- coding: utf-8 -*-
"""MRC_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D0cveLabP1nYyB1wGipzScvtj7ybbd0S
"""

"""## Imports and multi-worker config"""

from preprocessing import normalize, process
from model import create_model
from contextlib import redirect_stdout
from plotting import save_loss_accuracy_plots
from save_predict_plot import save_predict_plot
from tensorflow.keras.callbacks import CSVLogger
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import LearningRateScheduler
import tensorflow_addons as tfa
import tensorflow as tf
import io
import time
import numpy as np
import pandas as pd
import json
import os
import logging
import multiprocessing
import subprocess
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

"""## Constants"""

model_params_file = "./parameters/params.json"
technical_indicators_json = "./parameters/indicators.json"
timestamp = time.strftime("%Y%m%d-%H%M%S")

# Load parameters from the JSON files
with open(model_params_file, 'r') as file:
    params = json.load(file)
with open(technical_indicators_json, 'r') as f:
    indicator_config = json.load(f)

# Access the parameters
filename = params['filename']
window_size = params['window_size']
num_features = params['num_features']
batch_size = params['batch_size']
epochs = params['epochs']
initial_learning_rate = params['initial_learning_rate']
filter_size = params['filter_size']
external_filter_size = params['external_filter_size']
l2_reg = params['l2_reg']
bidirectional = params['bidirectional']
interval = params['interval']
train_size = params['train_size']
validation_size = params['validation_size']
test_size = params['test_size']
nr_of_labels = params['nr_of_labels']
modifier = params['modifier']
es_patience = params['es_patience']
gru = params['gru']
max_learning_rate_cycle = params['max_learning_rate_cycle']
initial_learning_rate_cycle = params['initial_learning_rate_cycle']
cycle = params['cycle']

assert (train_size + test_size + validation_size) == 1

base_results_path = './results'
run_path = os.path.join(base_results_path, timestamp)
os.makedirs(run_path)

"""## Multithread process status observer"""

def run_watcher_script(arg1):
    subprocess.run(["python3", "notifier.py", arg1])

arg1 = str(os.getpid())

# Create a new process for running the status checker
checker_process = multiprocessing.Process(target=run_watcher_script, args=(arg1,))
checker_process.start()

"""## Loading data from CSV"""

data = pd.read_csv(filename)

"""## Preprocessing"""

technical_indicators = []
for indicator, enabled in indicator_config.items():
    if enabled:
        technical_indicators.append(indicator)

print("Selected technical indicators:", technical_indicators)

#Creating file folder for normalization memory
normalization_path = os.path.join(run_path, "normalization")
os.makedirs(normalization_path)
normalization_file_path = os.path.join(normalization_path, "normalization.json")

data = process(data, modifier, technical_indicators)
plotting_data = data
data = normalize(data, technical_indicators, normalization_file_path)
data.drop(columns=['unix', 'close'], inplace=True) 


"""## Reshaping to fit convolutional layers"""

num_samples = len(data) - window_size + 1
dataset = tf.data.Dataset.from_tensor_slices(data)

# window[from 1 to windowsize -1, all columns except for the last two] window[only the last one, last two columns]
dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))
dataset = dataset.map(lambda window: (window[:-1, :-nr_of_labels], tf.squeeze(window[-1:, -nr_of_labels:])))

"""## Split the data"""

# Splitting into training and validation sets
train_size = int(train_size * num_samples)
validation_size = int(validation_size * num_samples)
test_size = int(test_size * num_samples)

train_dataset = dataset.take(train_size)
val_dataset = dataset.skip(train_size).take(validation_size)
test_dataset = dataset.skip(train_size).skip(validation_size)

test_input_data = np.array([data[0] for data in test_dataset])
# Train Data shuffle on each iteration inside batches
train_dataset = train_dataset.shuffle(train_size, reshuffle_each_iteration=True)

# Grouping windows into batches
train_dataset = train_dataset.batch(batch_size)
val_dataset = val_dataset.batch(batch_size)
test_dataset = test_dataset.batch(batch_size)

# Train batches Data shuffle on each iteration
train_dataset = train_dataset.shuffle(int(train_size/batch_size), reshuffle_each_iteration=True)

"""## Model object and strategy"""

model = create_model(window_size, num_features, bidirectional, filter_size, l2_reg, external_filter_size, nr_of_labels, gru)

loss = tf.keras.losses.BinaryCrossentropy()

if not cycle:
    optimizer = tf.keras.optimizers.Adam(
        learning_rate=initial_learning_rate
    )
else:
    steps_per_epoch = train_size

    clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=initial_learning_rate_cycle,
        maximal_learning_rate=max_learning_rate_cycle,
        scale_fn=lambda x: 1/(2.**(x-1)),
        step_size=2 * steps_per_epoch
    )
    optimizer = tf.keras.optimizers.SGD(clr)



model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'], run_eagerly=True)

"""## Logging, saving and plotting"""

# Model summary
summary_file = os.path.join(run_path, 'model_summary.txt')

buffer = io.StringIO()
with redirect_stdout(buffer):
    model.summary()
summary_string = buffer.getvalue()

with open(summary_file, 'w') as file:
    model_summary = model.summary()

    file.write(summary_string)

# Checkpoints
checkpoint_path = os.path.join(run_path, 'checkpoints')
os.makedirs(checkpoint_path)
checkpoint_file_path = os.path.join(checkpoint_path, 'checkpoint.h5')

# Logs
logs_path = os.path.join(run_path, "logs")
os.makedirs(logs_path)
logs_file_path = os.path.join(logs_path, "logs.log")

logging.basicConfig(filename=logs_file_path, level=logging.DEBUG)
logging.getLogger().addHandler(logging.FileHandler(logs_file_path))

# Training progress
progress_path = os.path.join(run_path, "progress")
os.makedirs(progress_path)
progress_file_path = os.path.join(progress_path, "progress.log")

# Parameters
params_path = os.path.join(run_path, "params")
os.makedirs(params_path)
params_path_file = os.path.join(params_path, "params.json")
command = f'cp {model_params_file} {params_path_file}'
os.popen(command)

# Technical indicators
technical_indicators_path_file = os.path.join(params_path, "indicators.json")
command = f'cp {technical_indicators_json} {technical_indicators_path_file}'
os.popen(command)
technical_indicators_file_path = os.path.join(params_path, "indicators.txt")
with open(technical_indicators_file_path, 'w') as file:
    for item in technical_indicators:
        file.write(item + '\n')

"""## Callbacks"""
if not cycle:
    # Define the learning rate schedule
    lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries=[50, 100, 150, 200, 250,300,350,400],
        # boundaries=[40,120,220, 300],
        values=[initial_learning_rate, initial_learning_rate * 0.5, initial_learning_rate * 0.5 * 0.5,
                initial_learning_rate * 0.5 * 0.5 * 0.5, initial_learning_rate * 0.5 * 0.5 * 0.5 * 0.5 , initial_learning_rate * 0.5 * 0.5 * 0.5 * 0.5 * 0.5,
                initial_learning_rate * 0.5 * 0.5 * 0.5 * 0.5 * 0.5 * 0.5, initial_learning_rate * 0.5 * 0.5 * 0.5 * 0.5 * 0.5 * 0.5 * 0.5, initial_learning_rate * 0.5 * 0.5 * 0.5 * 0.5 * 0.5 * 0.5 * 0.5 * 0.5]
    )

# Set the early stopping criteria
early_stopping = EarlyStopping(monitor='val_loss', patience=es_patience, restore_best_weights=True)

# Define a custom callback to clear GPU memory


class ClearMemoryCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        tf.keras.backend.clear_session()


# Create an instance of the custom callback
clear_memory_callback = ClearMemoryCallback()

# Set checkpoint callback
checkpoint_callback = ModelCheckpoint(  # Dodac zapisywanie checkpoint√≥w co zmiane lr
    filepath=checkpoint_file_path,
    save_weights_only=True,
    save_freq='epoch',
    period=5  # Save every 5 epochs
)

# Create the CSVLogger callback to save training logs
csv_logger_callback = CSVLogger(progress_file_path)

# Define a custom Early stopping counter callback
class BestEpochTrackerCallback(tf.keras.callbacks.Callback):
    def __init__(self):
        super(BestEpochTrackerCallback, self).__init__()
        self.best_epoch = 0
        self.best_val_loss = float('inf')
        
    def on_epoch_end(self, epoch, logs=None):
        current_val_loss = logs.get('val_loss')
        if current_val_loss < self.best_val_loss:
            self.best_val_loss = current_val_loss
            self.best_epoch = epoch
            print(f"Epoch {epoch} - Best Validation Loss: {current_val_loss}")

# Set counter callback
counter_callback = BestEpochTrackerCallback()

"""## Learning process"""

# Devices
physical_devices = tf.config.list_physical_devices('GPU')
print(physical_devices)
try:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
except:
    # Invalid device or cannot modify virtual devices once initialized.
    pass


callbacks=[clear_memory_callback, early_stopping, checkpoint_callback, csv_logger_callback]
if not cycle:
    callbacks.append(tf.keras.callbacks.LearningRateScheduler(lr_schedule))

# Train the model with learning rate schedule and early stopping

history = model.fit(train_dataset, validation_data=val_dataset, epochs=epochs, callbacks=callbacks)

"""## Plotting and saving"""

# Testing
test_loss, test_accuracy = model.evaluate(test_dataset)
print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
predictions = model.predict(test_input_data)

# Saving
model_path = os.path.join(run_path, 'model')
os.makedirs(model_path)
model_file_path = os.path.join(model_path, 'model.h5')
model.save(model_file_path)

# Plotting
plot_path = os.path.join(run_path, "plots")
os.makedirs(plot_path)
save_loss_accuracy_plots(history, plot_path)
save_predict_plot(plotting_data, plot_path, predictions)

exit()
